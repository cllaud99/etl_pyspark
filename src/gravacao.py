from pyspark.sql import SparkSession, DataFrame
import logging
from typing import Optional, Union, List, Dict, Any

logger = logging.getLogger(__name__)


class Gravacao:
    """
    Classe respons√°vel pela grava√ß√£o de dados em arquivos usando PySpark.

    Essa classe abstrai a l√≥gica de grava√ß√£o de DataFrames, permitindo o controle de:
    - Tipo de arquivo (parquet, csv, json, etc.)
    - N√∫mero de parti√ß√µes de sa√≠da
    - Particionamento f√≠sico por colunas
    """

    def __init__(self, spark: SparkSession, atributos: Dict[str, Any]) -> None:
        """
        Inicializa a classe Gravacao com as configura√ß√µes fornecidas.

        Args:
            spark (SparkSession): Sess√£o Spark ativa.
            atributos (dict): Dicion√°rio de configura√ß√£o contendo informa√ß√µes de destino,
                tipicamente carregado de um arquivo YAML. Exemplo:
                {
                    "destiny": {
                        "file_type": "parquet",
                        "file_path": "/caminho/saida/",
                        "options": {"header": True}
                    }
                }
        """
        # Guarda a sess√£o Spark para leitura e grava√ß√£o
        self.spark: SparkSession = spark

        # Guarda o dicion√°rio de configura√ß√£o carregado do YAML
        self.atributos: Dict[str, Any] = atributos

        # Tipo de arquivo de sa√≠da (ex: 'json', 'csv', 'parquet', etc.)
        self.tipo_destiny: str = atributos["destiny"].get("file_type", "").lower()
        print(f"self.tipo_destiny: {self.tipo_destiny} ############################")

        # Op√ß√µes adicionais para escrita (ex: header, delimiter, compression, etc.)
        self.options_destiny: Dict[str, Any] = atributos["destiny"].get("options", {})

        # Caminho do arquivo de destino
        self.caminho_destiny: str = atributos["destiny"]["file_path"]

        # DataFrame a ser gravado (atributo armazenado ap√≥s chamada do m√©todo gravar)
        self.df: Optional[DataFrame] = None

    def gravar(
        self,
        df: DataFrame,
        num_particoes: Optional[int] = None,
        particionar_por: Optional[Union[str, List[str]]] = None,
    ) -> None:
        """
        Grava um DataFrame em arquivo com controle de particionamento e formato.

        Este m√©todo permite gravar arquivos √∫nicos, m√∫ltiplos ou particionados por colunas.
        Suporta os formatos: parquet, csv, json e xml.

        Args:
            df (DataFrame): DataFrame a ser gravado.
            num_particoes (int, optional): Define o n√∫mero de parti√ß√µes de sa√≠da.
                - Se `1`: Gera um √∫nico arquivo (usa `.coalesce(1)`).
                - Se `> 1`: Gera m√∫ltiplos arquivos (usa `.repartition(n)`).
                - Se `None`: Usa particionamento padr√£o do Spark.
            particionar_por (str | list[str], optional): Nome(s) de coluna(s) para particionamento f√≠sico.
                Cria subpastas com base nos valores √∫nicos dessas colunas.

        Returns:
            None

        Raises:
            Exception: Caso ocorra erro durante a grava√ß√£o do DataFrame.

        Exemplo:
            >>> gravador.gravar(df, num_particoes=1)
            >>> gravador.gravar(df, num_particoes=4)
            >>> gravador.gravar(df, particionar_por="ANO")
        """
        self.df = df

        try:
            logger.info(f"üíæ Preparando grava√ß√£o em {self.caminho_destiny}")

            # Aplicar controle de particionamento, se especificado
            if num_particoes == 1:
                logger.info("üìÑ Gerando arquivo √∫nico (coalesce)...")
                self.df = self.df.coalesce(1)
            elif num_particoes and num_particoes > 1:
                logger.info(f"üìë Dividindo em {num_particoes} parti√ß√µes (repartition)...")
                self.df = self.df.repartition(num_particoes)
            else:
                logger.info("üìä Usando particionamento padr√£o do Spark...")

            # Valida√ß√£o do formato
            if self.tipo_destiny not in ("csv", "xml", "parquet", "json"):
                raise ValueError(
                    f"Tipo de arquivo '{self.tipo_destiny}' n√£o suportado. "
                    "Use 'csv', 'json', 'parquet' ou 'xml'."
                )

            # Configura√ß√£o inicial do writer
            writer = self.df.write.format(self.tipo_destiny).mode("overwrite")

            # Adiciona particionamento f√≠sico, se solicitado
            if particionar_por:
                if isinstance(particionar_por, str):
                    particionar_por = [particionar_por]
                logger.info(f"üìÇ Particionando por: {', '.join(particionar_por)}")
                writer = writer.partitionBy(*particionar_por)

            # Aplicar op√ß√µes adicionais e gravar
            writer.options(**self.options_destiny).save(self.caminho_destiny)
            logger.info("‚úÖ Dados gravados com sucesso!")

        except Exception as e:
            logger.error(f"‚ùå Erro ao gravar dados: {e}")
            raise
